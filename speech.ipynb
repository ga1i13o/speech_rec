{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T20:29:39.474904Z",
     "start_time": "2020-10-22T20:29:38.854338Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "checkpoint_cer = \"saved_models/base_model_bestCer.pth\"\n",
    "checkpoint_wer = \"saved_models/base_model_bestWer.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T20:29:39.816279Z",
     "start_time": "2020-10-22T20:29:39.476358Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"train-clean-100\", folder_in_archive=\"LibriSpeech\")\n",
    "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"test-clean\", folder_in_archive=\"LibriSpeech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T10:40:05.329662Z",
     "start_time": "2020-10-19T10:40:05.325907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2620"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T10:40:06.912401Z",
     "start_time": "2020-10-19T10:40:06.909257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28539"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T20:51:35.034410Z",
     "start_time": "2020-10-20T20:51:35.031238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T19:21:18.530029Z",
     "start_time": "2020-10-18T19:21:18.526718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()\n",
    "cuda = torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T20:29:41.282441Z",
     "start_time": "2020-10-22T20:29:41.268979Z"
    }
   },
   "outputs": [],
   "source": [
    "def avg_wer(wer_scores, combined_ref_len):\n",
    "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
    "\n",
    "\n",
    "def _levenshtein_distance(ref, hyp):\n",
    "    \"\"\"Levenshtein distance is a string metric for measuring the difference\n",
    "    between two sequences. Informally, the levenshtein disctance is defined as\n",
    "    the minimum number of single-character edits (substitutions, insertions or\n",
    "    deletions) required to change one word into the other. We can naturally\n",
    "    extend the edits to word level when calculate levenshtein disctance for\n",
    "    two sentences.\n",
    "    \"\"\"\n",
    "    m = len(ref)\n",
    "    n = len(hyp)\n",
    "\n",
    "    # special case\n",
    "    if ref == hyp:\n",
    "        return 0\n",
    "    if m == 0:\n",
    "        return n\n",
    "    if n == 0:\n",
    "        return m\n",
    "\n",
    "    if m < n:\n",
    "        ref, hyp = hyp, ref\n",
    "        m, n = n, m\n",
    "\n",
    "    # use O(min(m, n)) space\n",
    "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
    "\n",
    "    # initialize distance matrix\n",
    "    for j in range(0,n + 1):\n",
    "        distance[0][j] = j\n",
    "\n",
    "    # calculate levenshtein distance\n",
    "    for i in range(1, m + 1):\n",
    "        prev_row_idx = (i - 1) % 2\n",
    "        cur_row_idx = i % 2\n",
    "        distance[cur_row_idx][0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            if ref[i - 1] == hyp[j - 1]:\n",
    "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
    "            else:\n",
    "                s_num = distance[prev_row_idx][j - 1] + 1\n",
    "                i_num = distance[cur_row_idx][j - 1] + 1\n",
    "                d_num = distance[prev_row_idx][j] + 1\n",
    "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
    "\n",
    "    return distance[m % 2][n]\n",
    "\n",
    "\n",
    "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
    "    hypothesis sequence in word-level.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param delimiter: Delimiter of input sentences.\n",
    "    :type delimiter: char\n",
    "    :return: Levenshtein distance and word number of reference sentence.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "    ref_words = reference.split(delimiter)\n",
    "    hyp_words = hypothesis.split(delimiter)\n",
    "\n",
    "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
    "    return float(edit_distance), len(ref_words)\n",
    "\n",
    "\n",
    "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
    "    hypothesis sequence in char-level.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param remove_space: Whether remove internal space characters\n",
    "    :type remove_space: bool\n",
    "    :return: Levenshtein distance and length of reference sentence.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "    join_char = ' '\n",
    "    if remove_space == True:\n",
    "        join_char = ''\n",
    "\n",
    "    reference = join_char.join(filter(None, reference.split(' ')))\n",
    "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
    "\n",
    "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
    "    return float(edit_distance), len(reference)\n",
    "\n",
    "\n",
    "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    \"\"\"Calculate word error rate (WER). WER compares reference text and\n",
    "    hypothesis text in word-level. WER is defined as:\n",
    "    .. math::\n",
    "        WER = (Sw + Dw + Iw) / Nw\n",
    "    where\n",
    "    .. code-block:: text\n",
    "        Sw is the number of words subsituted,\n",
    "        Dw is the number of words deleted,\n",
    "        Iw is the number of words inserted,\n",
    "        Nw is the number of words in the reference\n",
    "    We can use levenshtein distance to calculate WER. Please draw an attention\n",
    "    that empty items will be removed when splitting sentences by delimiter.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param delimiter: Delimiter of input sentences.\n",
    "    :type delimiter: char\n",
    "    :return: Word error rate.\n",
    "    :rtype: float\n",
    "    :raises ValueError: If word number of reference is zero.\n",
    "    \"\"\"\n",
    "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
    "                                         delimiter)\n",
    "\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
    "\n",
    "    wer = float(edit_distance) / ref_len\n",
    "    return wer\n",
    "\n",
    "\n",
    "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n",
    "    hypothesis text in char-level. CER is defined as:\n",
    "    .. math::\n",
    "        CER = (Sc + Dc + Ic) / Nc\n",
    "    where\n",
    "    .. code-block:: text\n",
    "        Sc is the number of characters substituted,\n",
    "        Dc is the number of characters deleted,\n",
    "        Ic is the number of characters inserted\n",
    "        Nc is the number of characters in the reference\n",
    "    We can use levenshtein distance to calculate CER. Chinese input should be\n",
    "    encoded to unicode. Please draw an attention that the leading and tailing\n",
    "    space characters will be truncated and multiple consecutive space\n",
    "    characters in a sentence will be replaced by one space character.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param remove_space: Whether remove internal space characters\n",
    "    :type remove_space: bool\n",
    "    :return: Character error rate.\n",
    "    :rtype: float\n",
    "    :raises ValueError: If the reference length is zero.\n",
    "    \"\"\"\n",
    "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
    "                                         remove_space)\n",
    "\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
    "\n",
    "    cer = float(edit_distance) / ref_len\n",
    "    return cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T20:29:42.100676Z",
     "start_time": "2020-10-22T20:29:42.067857Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "        char_map_str = \"\"\"\n",
    "        ' 0\n",
    "        <SPACE> 1\n",
    "        a 2\n",
    "        b 3\n",
    "        c 4\n",
    "        d 5\n",
    "        e 6\n",
    "        f 7\n",
    "        g 8\n",
    "        h 9\n",
    "        i 10\n",
    "        j 11\n",
    "        k 12\n",
    "        l 13\n",
    "        m 14\n",
    "        n 15\n",
    "        o 16\n",
    "        p 17\n",
    "        q 18\n",
    "        r 19\n",
    "        s 20\n",
    "        t 21\n",
    "        u 22\n",
    "        v 23\n",
    "        w 24\n",
    "        x 25\n",
    "        y 26\n",
    "        z 27\n",
    "        \"\"\"\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "        self.index_map[1] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['<SPACE>']\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string).replace('<SPACE>', ' ')\n",
    "\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "text_transform = TextTransform()\n",
    "\n",
    "\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        spectrograms.append(spec)\n",
    "        \n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T18:24:47.155783Z",
     "start_time": "2020-08-14T18:24:47.146961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = []\n",
    "ex.append((train_dataset[0]))\n",
    "type(ex)\n",
    "len(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T20:29:43.795646Z",
     "start_time": "2020-10-22T20:29:43.780466Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n",
    "\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T20:29:44.938020Z",
     "start_time": "2020-10-22T20:29:44.931689Z"
    }
   },
   "outputs": [],
   "source": [
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data \n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(spectrograms)  # (batch, time, n_class)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "        \n",
    "        #print(f\"EPOCH {epoch}, STEP {iter_meter.get()}, LR {scheduler.get_last_lr()}\\n\\t loss : {loss}\")\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        iter_meter.step()\n",
    "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(spectrograms), data_len,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T20:32:01.454674Z",
     "start_time": "2020-10-22T20:32:01.444507Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, epoch, iter_meter):\n",
    "    global best_cer\n",
    "    global best_wer\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    \n",
    "    load_times = []\n",
    "    forward_pass_times = []\n",
    "    eval_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data\n",
    "            start = time.time()\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            elaps = time.time() - start\n",
    "            load_times.append(elaps)\n",
    "            \n",
    "            start = time.time()\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "            elaps = time.time() - start\n",
    "            forward_pass_times.append(elaps)\n",
    "            \n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "    \n",
    "            start = time.time()\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "            elaps = time.time() - start\n",
    "            eval_times.append(elaps)\n",
    "            \n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "    \n",
    "    tot = sum(load_times) + sum(forward_pass_times) + sum(eval_times)\n",
    "    loads = sum(load_times)/tot\n",
    "    forw = sum(forward_pass_times)/tot\n",
    "    evl = sum(eval_times)/tot\n",
    "    print(f\"elapsed time: {tot/60:4.3f} mins\\n\\tloads: {loads:4.3f},\\n\\tforward: {forw:4.3f},\\n\\teval: {evl:4.3f}\")\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:.4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
    "    \n",
    "    if avg_cer < best_cer:\n",
    "        print(f\"\\n[NEW BEST CER] -- {avg_cer} @ loss {test_loss}\")\n",
    "        best_cer = avg_cer\n",
    "        torch.save(model, checkpoint_cer)\n",
    "    elif avg_wer < best_wer:\n",
    "        print(f\"\\n[NEW BEST WER] -- {avg_wer} @ loss {test_loss}\")\n",
    "        best_wer = avg_wer\n",
    "        torch.save(model, checkpoint_wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T20:32:06.908928Z",
     "start_time": "2020-10-22T20:32:06.900242Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(learning_rate=5e-4, batch_size=20, epochs=10, train_url=\"train-clean-100\", test_url=\"test-clean\", use_model=\"new\"):\n",
    "\n",
    "    hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 29,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=train_url, folder_in_archive=\"LibriSpeech\")\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=test_url, folder_in_archive=\"LibriSpeech\")\n",
    "\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "    if use_model == 'new':\n",
    "        model = SpeechRecognitionModel(\n",
    "            hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "            hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "            ).to(device)\n",
    "    else:\n",
    "        model = torch.load(checkpoint_cer).to(device)\n",
    "\n",
    "    print(model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = nn.CTCLoss(blank=28).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "    \n",
    "    iter_meter = IterMeter()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n",
    "        test(model, device, test_loader, criterion, epoch, iter_meter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T22:46:15.580228Z",
     "start_time": "2020-10-22T20:32:14.709822Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utente/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/cuda/__init__.py:125: UserWarning: \n",
      "GeForce GT 710 with CUDA capability sm_35 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the GeForce GT 710 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechRecognitionModel(\n",
      "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (rescnn_layers): Sequential(\n",
      "    (0): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (birnn_layers): Sequential(\n",
      "    (0): BidirectionalGRU(\n",
      "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): GELU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
      "  )\n",
      ")\n",
      "Num Model Parameters 23705373\n",
      "Train Epoch: 1 [0/28539 (0%)]\tLoss: 0.478218\n",
      "Train Epoch: 1 [1600/28539 (6%)]\tLoss: 0.463054\n",
      "Train Epoch: 1 [3200/28539 (11%)]\tLoss: 0.395300\n",
      "Train Epoch: 1 [4800/28539 (17%)]\tLoss: 0.405483\n",
      "Train Epoch: 1 [6400/28539 (22%)]\tLoss: 0.346619\n",
      "Train Epoch: 1 [8000/28539 (28%)]\tLoss: 0.409233\n",
      "Train Epoch: 1 [9600/28539 (34%)]\tLoss: 0.425799\n",
      "Train Epoch: 1 [11200/28539 (39%)]\tLoss: 0.400158\n",
      "Train Epoch: 1 [12800/28539 (45%)]\tLoss: 0.398609\n",
      "Train Epoch: 1 [14400/28539 (50%)]\tLoss: 0.378632\n",
      "Train Epoch: 1 [16000/28539 (56%)]\tLoss: 0.434506\n",
      "Train Epoch: 1 [17600/28539 (62%)]\tLoss: 0.358813\n",
      "Train Epoch: 1 [19200/28539 (67%)]\tLoss: 0.393826\n",
      "Train Epoch: 1 [20800/28539 (73%)]\tLoss: 0.361379\n",
      "Train Epoch: 1 [22400/28539 (78%)]\tLoss: 0.264210\n",
      "Train Epoch: 1 [24000/28539 (84%)]\tLoss: 0.406538\n",
      "Train Epoch: 1 [25600/28539 (90%)]\tLoss: 0.352992\n",
      "Train Epoch: 1 [27200/28539 (95%)]\tLoss: 0.382756\n",
      "\n",
      "evaluating...\n",
      "elapsed time: 5.951 mins\n",
      "\tloads: 0.001,\n",
      "\tforward: 0.028,\n",
      "\teval: 0.971\n",
      "Test set: Average loss: 0.5047, Average CER: 0.1385 Average WER: 0.4178\n",
      "\n",
      "\n",
      "[NEW BEST CER] -- 0.1384892068103307 @ loss 0.5047030651533023\n",
      "Train Epoch: 2 [0/28539 (0%)]\tLoss: 0.340771\n",
      "Train Epoch: 2 [1600/28539 (6%)]\tLoss: 0.353840\n",
      "Train Epoch: 2 [3200/28539 (11%)]\tLoss: 0.322885\n",
      "Train Epoch: 2 [4800/28539 (17%)]\tLoss: 0.366225\n",
      "Train Epoch: 2 [6400/28539 (22%)]\tLoss: 0.343671\n",
      "Train Epoch: 2 [8000/28539 (28%)]\tLoss: 0.345366\n",
      "Train Epoch: 2 [9600/28539 (34%)]\tLoss: 0.331406\n",
      "Train Epoch: 2 [11200/28539 (39%)]\tLoss: 0.432155\n",
      "Train Epoch: 2 [12800/28539 (45%)]\tLoss: 0.346356\n",
      "Train Epoch: 2 [14400/28539 (50%)]\tLoss: 0.332803\n",
      "Train Epoch: 2 [16000/28539 (56%)]\tLoss: 0.351625\n",
      "Train Epoch: 2 [17600/28539 (62%)]\tLoss: 0.452640\n",
      "Train Epoch: 2 [19200/28539 (67%)]\tLoss: 0.425328\n",
      "Train Epoch: 2 [20800/28539 (73%)]\tLoss: 0.354617\n",
      "Train Epoch: 2 [22400/28539 (78%)]\tLoss: 0.369937\n",
      "Train Epoch: 2 [24000/28539 (84%)]\tLoss: 0.355077\n",
      "Train Epoch: 2 [25600/28539 (90%)]\tLoss: 0.329790\n",
      "Train Epoch: 2 [27200/28539 (95%)]\tLoss: 0.315677\n",
      "\n",
      "evaluating...\n",
      "elapsed time: 6.167 mins\n",
      "\tloads: 0.001,\n",
      "\tforward: 0.026,\n",
      "\teval: 0.973\n",
      "Test set: Average loss: 0.4976, Average CER: 0.1367 Average WER: 0.4115\n",
      "\n",
      "\n",
      "[NEW BEST CER] -- 0.1366854490201095 @ loss 0.4976229075251557\n",
      "Train Epoch: 3 [0/28539 (0%)]\tLoss: 0.349302\n",
      "Train Epoch: 3 [1600/28539 (6%)]\tLoss: 0.419042\n",
      "Train Epoch: 3 [3200/28539 (11%)]\tLoss: 0.343518\n",
      "Train Epoch: 3 [4800/28539 (17%)]\tLoss: 0.335870\n",
      "Train Epoch: 3 [6400/28539 (22%)]\tLoss: 0.372969\n",
      "Train Epoch: 3 [8000/28539 (28%)]\tLoss: 0.350594\n",
      "Train Epoch: 3 [9600/28539 (34%)]\tLoss: 0.332753\n",
      "Train Epoch: 3 [11200/28539 (39%)]\tLoss: 0.379402\n",
      "Train Epoch: 3 [12800/28539 (45%)]\tLoss: 0.336237\n",
      "Train Epoch: 3 [14400/28539 (50%)]\tLoss: 0.289067\n",
      "Train Epoch: 3 [16000/28539 (56%)]\tLoss: 0.342695\n",
      "Train Epoch: 3 [17600/28539 (62%)]\tLoss: 0.299169\n",
      "Train Epoch: 3 [19200/28539 (67%)]\tLoss: 0.295767\n",
      "Train Epoch: 3 [20800/28539 (73%)]\tLoss: 0.347242\n",
      "Train Epoch: 3 [22400/28539 (78%)]\tLoss: 0.337520\n",
      "Train Epoch: 3 [24000/28539 (84%)]\tLoss: 0.337618\n",
      "Train Epoch: 3 [25600/28539 (90%)]\tLoss: 0.324140\n",
      "Train Epoch: 3 [27200/28539 (95%)]\tLoss: 0.326162\n",
      "\n",
      "evaluating...\n",
      "elapsed time: 6.197 mins\n",
      "\tloads: 0.001,\n",
      "\tforward: 0.026,\n",
      "\teval: 0.973\n",
      "Test set: Average loss: 0.5014, Average CER: 0.1368 Average WER: 0.4120\n",
      "\n",
      "\n",
      "[NEW BEST WER] -- 0.41197178064190537 @ loss 0.5014121682360404\n",
      "Train Epoch: 4 [0/28539 (0%)]\tLoss: 0.340047\n",
      "Train Epoch: 4 [1600/28539 (6%)]\tLoss: 0.333486\n",
      "Train Epoch: 4 [3200/28539 (11%)]\tLoss: 0.340337\n",
      "Train Epoch: 4 [4800/28539 (17%)]\tLoss: 0.295445\n",
      "Train Epoch: 4 [6400/28539 (22%)]\tLoss: 0.254982\n",
      "Train Epoch: 4 [8000/28539 (28%)]\tLoss: 0.322131\n",
      "Train Epoch: 4 [9600/28539 (34%)]\tLoss: 0.226889\n",
      "Train Epoch: 4 [11200/28539 (39%)]\tLoss: 0.361456\n",
      "Train Epoch: 4 [12800/28539 (45%)]\tLoss: 0.328601\n",
      "Train Epoch: 4 [14400/28539 (50%)]\tLoss: 0.295993\n",
      "Train Epoch: 4 [16000/28539 (56%)]\tLoss: 0.356974\n",
      "Train Epoch: 4 [17600/28539 (62%)]\tLoss: 0.345778\n",
      "Train Epoch: 4 [19200/28539 (67%)]\tLoss: 0.318357\n",
      "Train Epoch: 4 [20800/28539 (73%)]\tLoss: 0.251161\n",
      "Train Epoch: 4 [22400/28539 (78%)]\tLoss: 0.363732\n",
      "Train Epoch: 4 [24000/28539 (84%)]\tLoss: 0.287032\n",
      "Train Epoch: 4 [25600/28539 (90%)]\tLoss: 0.310020\n",
      "Train Epoch: 4 [27200/28539 (95%)]\tLoss: 0.237993\n",
      "\n",
      "evaluating...\n",
      "elapsed time: 6.030 mins\n",
      "\tloads: 0.001,\n",
      "\tforward: 0.027,\n",
      "\teval: 0.973\n",
      "Test set: Average loss: 0.5058, Average CER: 0.1349 Average WER: 0.4057\n",
      "\n",
      "\n",
      "[NEW BEST CER] -- 0.13485362704674864 @ loss 0.5058026476422463\n",
      "Train Epoch: 5 [0/28539 (0%)]\tLoss: 0.301011\n",
      "Train Epoch: 5 [1600/28539 (6%)]\tLoss: 0.306996\n",
      "Train Epoch: 5 [3200/28539 (11%)]\tLoss: 0.339360\n",
      "Train Epoch: 5 [4800/28539 (17%)]\tLoss: 0.242382\n",
      "Train Epoch: 5 [6400/28539 (22%)]\tLoss: 0.339032\n",
      "Train Epoch: 5 [8000/28539 (28%)]\tLoss: 0.284446\n",
      "Train Epoch: 5 [9600/28539 (34%)]\tLoss: 0.229784\n",
      "Train Epoch: 5 [11200/28539 (39%)]\tLoss: 0.423634\n",
      "Train Epoch: 5 [12800/28539 (45%)]\tLoss: 0.298503\n",
      "Train Epoch: 5 [14400/28539 (50%)]\tLoss: 0.265203\n",
      "Train Epoch: 5 [16000/28539 (56%)]\tLoss: 0.285052\n",
      "Train Epoch: 5 [17600/28539 (62%)]\tLoss: 0.321084\n",
      "Train Epoch: 5 [19200/28539 (67%)]\tLoss: 0.237894\n",
      "Train Epoch: 5 [20800/28539 (73%)]\tLoss: 0.300703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [22400/28539 (78%)]\tLoss: 0.293734\n",
      "Train Epoch: 5 [24000/28539 (84%)]\tLoss: 0.312683\n",
      "Train Epoch: 5 [25600/28539 (90%)]\tLoss: 0.237031\n",
      "Train Epoch: 5 [27200/28539 (95%)]\tLoss: 0.329083\n",
      "\n",
      "evaluating...\n",
      "elapsed time: 6.165 mins\n",
      "\tloads: 0.001,\n",
      "\tforward: 0.026,\n",
      "\teval: 0.973\n",
      "Test set: Average loss: 0.5050, Average CER: 0.1352 Average WER: 0.4064\n",
      "\n",
      "\n",
      "[NEW BEST WER] -- 0.40644350013788655 @ loss 0.5049856726716204\n",
      "Train Epoch: 6 [0/28539 (0%)]\tLoss: 0.323910\n",
      "Train Epoch: 6 [1600/28539 (6%)]\tLoss: 0.388788\n",
      "Train Epoch: 6 [3200/28539 (11%)]\tLoss: 0.323529\n",
      "Train Epoch: 6 [4800/28539 (17%)]\tLoss: 0.312442\n",
      "Train Epoch: 6 [6400/28539 (22%)]\tLoss: 0.395398\n",
      "Train Epoch: 6 [8000/28539 (28%)]\tLoss: 0.296757\n",
      "Train Epoch: 6 [9600/28539 (34%)]\tLoss: 0.377247\n",
      "Train Epoch: 6 [11200/28539 (39%)]\tLoss: 0.352845\n",
      "Train Epoch: 6 [12800/28539 (45%)]\tLoss: 0.370526\n",
      "Train Epoch: 6 [14400/28539 (50%)]\tLoss: 0.263949\n",
      "Train Epoch: 6 [16000/28539 (56%)]\tLoss: 0.380191\n",
      "Train Epoch: 6 [17600/28539 (62%)]\tLoss: 0.360757\n",
      "Train Epoch: 6 [19200/28539 (67%)]\tLoss: 0.360348\n",
      "Train Epoch: 6 [20800/28539 (73%)]\tLoss: 0.359138\n",
      "Train Epoch: 6 [22400/28539 (78%)]\tLoss: 0.301335\n",
      "Train Epoch: 6 [24000/28539 (84%)]\tLoss: 0.345099\n",
      "Train Epoch: 6 [25600/28539 (90%)]\tLoss: 0.359152\n",
      "Train Epoch: 6 [27200/28539 (95%)]\tLoss: 0.325051\n",
      "\n",
      "evaluating...\n",
      "elapsed time: 6.156 mins\n",
      "\tloads: 0.001,\n",
      "\tforward: 0.027,\n",
      "\teval: 0.973\n",
      "Test set: Average loss: 0.4846, Average CER: 0.1331 Average WER: 0.4004\n",
      "\n",
      "\n",
      "[NEW BEST CER] -- 0.13311553957789796 @ loss 0.4846250596569806\n",
      "Train Epoch: 7 [0/28539 (0%)]\tLoss: 0.321257\n",
      "Train Epoch: 7 [1600/28539 (6%)]\tLoss: 0.313475\n",
      "Train Epoch: 7 [3200/28539 (11%)]\tLoss: 0.357612\n",
      "Train Epoch: 7 [4800/28539 (17%)]\tLoss: 0.340858\n",
      "Train Epoch: 7 [6400/28539 (22%)]\tLoss: 0.350082\n",
      "Train Epoch: 7 [8000/28539 (28%)]\tLoss: 0.323591\n",
      "Train Epoch: 7 [9600/28539 (34%)]\tLoss: 0.304127\n",
      "Train Epoch: 7 [11200/28539 (39%)]\tLoss: 0.322189\n",
      "Train Epoch: 7 [12800/28539 (45%)]\tLoss: 0.377224\n",
      "Train Epoch: 7 [14400/28539 (50%)]\tLoss: 0.262601\n",
      "Train Epoch: 7 [16000/28539 (56%)]\tLoss: 0.307027\n",
      "Train Epoch: 7 [17600/28539 (62%)]\tLoss: 0.356197\n",
      "Train Epoch: 7 [19200/28539 (67%)]\tLoss: 0.398868\n",
      "Train Epoch: 7 [20800/28539 (73%)]\tLoss: 0.320118\n",
      "Train Epoch: 7 [22400/28539 (78%)]\tLoss: 0.259647\n",
      "Train Epoch: 7 [24000/28539 (84%)]\tLoss: 0.262239\n",
      "Train Epoch: 7 [25600/28539 (90%)]\tLoss: 0.329438\n",
      "Train Epoch: 7 [27200/28539 (95%)]\tLoss: 0.328802\n",
      "\n",
      "evaluating...\n",
      "elapsed time: 6.133 mins\n",
      "\tloads: 0.001,\n",
      "\tforward: 0.027,\n",
      "\teval: 0.973\n",
      "Test set: Average loss: 0.4862, Average CER: 0.1314 Average WER: 0.3972\n",
      "\n",
      "\n",
      "[NEW BEST CER] -- 0.1314263182899007 @ loss 0.4861805060106078\n",
      "Train Epoch: 8 [0/28539 (0%)]\tLoss: 0.298559\n",
      "Train Epoch: 8 [1600/28539 (6%)]\tLoss: 0.285409\n",
      "Train Epoch: 8 [3200/28539 (11%)]\tLoss: 0.289020\n",
      "Train Epoch: 8 [4800/28539 (17%)]\tLoss: 0.286859\n",
      "Train Epoch: 8 [6400/28539 (22%)]\tLoss: 0.311200\n",
      "Train Epoch: 8 [8000/28539 (28%)]\tLoss: 0.257527\n",
      "Train Epoch: 8 [9600/28539 (34%)]\tLoss: 0.300245\n",
      "Train Epoch: 8 [11200/28539 (39%)]\tLoss: 0.317617\n",
      "Train Epoch: 8 [12800/28539 (45%)]\tLoss: 0.294012\n",
      "Train Epoch: 8 [14400/28539 (50%)]\tLoss: 0.324537\n",
      "Train Epoch: 8 [16000/28539 (56%)]\tLoss: 0.315180\n",
      "Train Epoch: 8 [17600/28539 (62%)]\tLoss: 0.274397\n",
      "Train Epoch: 8 [19200/28539 (67%)]\tLoss: 0.305963\n",
      "Train Epoch: 8 [20800/28539 (73%)]\tLoss: 0.323448\n",
      "Train Epoch: 8 [22400/28539 (78%)]\tLoss: 0.293217\n",
      "Train Epoch: 8 [24000/28539 (84%)]\tLoss: 0.366371\n",
      "Train Epoch: 8 [25600/28539 (90%)]\tLoss: 0.310175\n",
      "Train Epoch: 8 [27200/28539 (95%)]\tLoss: 0.388427\n",
      "\n",
      "evaluating...\n",
      "elapsed time: 6.161 mins\n",
      "\tloads: 0.001,\n",
      "\tforward: 0.026,\n",
      "\teval: 0.973\n",
      "Test set: Average loss: 0.4842, Average CER: 0.1312 Average WER: 0.3952\n",
      "\n",
      "\n",
      "[NEW BEST CER] -- 0.13116435969478135 @ loss 0.48423372445310015\n",
      "Train Epoch: 9 [0/28539 (0%)]\tLoss: 0.303171\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f337c05ca952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbest_wer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_test_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'old'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-c2e592c738aa>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(learning_rate, batch_size, epochs, train_url, test_url, use_model)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0miter_meter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_meter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_meter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b2be7417297a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (time, batch, n_class)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n\u001b[0m\u001b[1;32m   1360\u001b[0m                           self.zero_infinity)\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \"\"\"\n\u001b[0;32m-> 2154\u001b[0;31m     return torch.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank, _Reduction.get_enum(reduction),\n\u001b[0m\u001b[1;32m   2155\u001b[0m                           zero_infinity)\n\u001b[1;32m   2156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "libri_train_set = \"train-clean-100\"\n",
    "libri_test_set = \"test-clean\"\n",
    "best_cer = 1.0\n",
    "best_wer = 1.0\n",
    "\n",
    "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set, use_model='old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T23:44:19.211515Z",
     "start_time": "2020-08-15T23:44:17.964813Z"
    }
   },
   "outputs": [],
   "source": [
    "trained = torch.load(checkpoint_cer)\n",
    "trained.eval()\n",
    "trained = trained.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T23:44:19.218043Z",
     "start_time": "2020-08-15T23:44:19.212453Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"test-clean\", folder_in_archive=\"LibriSpeech\")\n",
    "\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} \n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                            batch_size=10,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                            **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T21:19:23.868438Z",
     "start_time": "2020-08-15T21:19:23.859235Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "test_cer, test_wer = [], []\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for i, _data in enumerate(test_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data \n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        output = model(spectrograms)  # (batch, time, n_class)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "        for j in range(len(decoded_preds)):\n",
    "            test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "            test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "\n",
    "avg_cer = sum(test_cer)/len(test_cer)\n",
    "avg_wer = sum(test_wer)/len(test_wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T23:44:21.264045Z",
     "start_time": "2020-08-15T23:44:21.052500Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, _data in enumerate(test_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data \n",
    "        spectrograms, labels = spectrograms.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        output = trained(spectrograms)  # (batch, time, n_class)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T23:39:24.948861Z",
     "start_time": "2020-08-15T23:39:24.940323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 128, 1149])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrograms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T23:44:23.318696Z",
     "start_time": "2020-08-15T23:44:23.309411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([575, 10, 29])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = F.log_softmax(output, dim=2)\n",
    "output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T23:44:24.918588Z",
     "start_time": "2020-08-15T23:44:24.908073Z"
    }
   },
   "outputs": [],
   "source": [
    "import ctcdecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T23:44:26.463235Z",
     "start_time": "2020-08-15T23:44:26.460541Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = [v for v in text_transform.index_map.values()]\n",
    "decoder = ctcdecode.CTCBeamDecoder(vocab, beam_width=5, blank_id=28)\n",
    "beam_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T23:39:33.409087Z",
     "start_time": "2020-08-15T23:39:33.405988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 29])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-15T23:44:36.172Z"
    }
   },
   "outputs": [],
   "source": [
    "beam_result, beam_scores, timesteps, out_seq_len = decoder.decode(probs_seq)\n",
    "print(\"aaa\")\n",
    "output_str = self.convert_to_string(beam_result[0][0], vocab, out_seq_len[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T23:44:35.440047Z",
     "start_time": "2020-08-15T23:44:35.436019Z"
    }
   },
   "outputs": [],
   "source": [
    "probs_seq1 = [[\n",
    "    0.06390443, 0.21124858, 0.27323887, 0.06870235, 0.0361254,\n",
    "    0.18184413, 0.16493624\n",
    "], [\n",
    "    0.03309247, 0.22866108, 0.24390638, 0.09699597, 0.31895462,\n",
    "    0.0094893, 0.06890021\n",
    "], [\n",
    "    0.218104, 0.19992557, 0.18245131, 0.08503348, 0.14903535,\n",
    "    0.08424043, 0.08120984\n",
    "], [\n",
    "    0.12094152, 0.19162472, 0.01473646, 0.28045061, 0.24246305,\n",
    "    0.05206269, 0.09772094\n",
    "], [\n",
    "    0.1333387, 0.00550838, 0.00301669, 0.21745861, 0.20803985,\n",
    "    0.41317442, 0.01946335\n",
    "], [\n",
    "    0.16468227, 0.1980699, 0.1906545, 0.18963251, 0.19860937,\n",
    "    0.04377724, 0.01457421\n",
    "]]\n",
    "probs_seq = torch.FloatTensor([probs_seq1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T23:35:01.042368Z",
     "start_time": "2020-08-15T23:35:01.039168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 29])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T21:24:33.146735Z",
     "start_time": "2020-08-15T21:24:32.930119Z"
    }
   },
   "outputs": [],
   "source": [
    "decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T21:26:49.221230Z",
     "start_time": "2020-08-15T21:26:49.218104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catlean wayof the towrch to anfrow as she recidsed ome beautiful lines ridtten for some such purpose as that which cralled them together to night'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_preds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T21:26:50.644691Z",
     "start_time": "2020-08-15T21:26:50.641724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kathleen waved the torch to and fro as she recited some beautiful lines written for some such purpose as that which called them together to night'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_targets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T21:25:24.213044Z",
     "start_time": "2020-08-15T21:25:22.642951Z"
    }
   },
   "outputs": [],
   "source": [
    "test_cer, test_wer = [], []\n",
    "for j in range(len(decoded_preds)):\n",
    "    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T21:26:44.548786Z",
     "start_time": "2020-08-15T21:26:44.545798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34615384615384615"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_wer[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
